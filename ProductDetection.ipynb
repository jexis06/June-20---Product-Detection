{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z66oAHm58sfH"
   },
   "source": [
    "## **Import the libraries we’ll need during our model building phase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "VhD4vh998ub-",
    "outputId": "f231d00d-8827-4fe0-dc3a-be6331c2711d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJ-3_ZINHosC"
   },
   "source": [
    "## **Load data from your local drive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XgepKKQWJyjm"
   },
   "source": [
    "Recall the pre-processing steps we discussed earlier. We’ll be using them here after loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVqCS567J08w"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWj3VokXSFpN"
   },
   "source": [
    "fromNext, we will read all the training images, store them in a list, and finally convert that list into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x85GXgLJSGdQ"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "train_image = []\n",
    "img_paths = []\n",
    "dim = 100\n",
    "\n",
    "for i in range(train.shape[0]):\n",
    "    img_paths.append('train\\\\train\\\\'+str(train['category'][i]).zfill(2)+'\\\\'+train['filename'][i])\n",
    "\n",
    "for path in img_paths:\n",
    "    img = image.load_img(path, grayscale=False, color_mode='grayscale', target_size=(dim,dim))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    train_image.append(img)\n",
    "    del img\n",
    "    gc.collect()\n",
    "\n",
    "X = np.array(train_image)\n",
    "\n",
    "#dispose not-to-be-used memory\n",
    "del train_image\n",
    "del img_paths\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UhOR73xutW4N"
   },
   "source": [
    "As it is a multi-class classification problem (42 classes), we will one-hot encode the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqCUfdRrtjjR"
   },
   "outputs": [],
   "source": [
    "y = train['category'].values\n",
    "y = to_categorical(y)\n",
    "\n",
    "#dispose not-to-be-used memory\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxZZ1Gr0umVr"
   },
   "source": [
    "## **Creating a validation set from the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ilt40x4Xuphb"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "#dispose not-to-be-used memory\n",
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PnQm2n9qvAnm"
   },
   "source": [
    "## **Define the model structure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ASCJrIhrvFQ_"
   },
   "source": [
    "We will create a simple architecture with 2 convolutional layers, one dense hidden layer and an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9wWwfG8yvHrU"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(dim,dim,1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(42, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VwZ3F9RMvT0T"
   },
   "source": [
    "Next, we will compile the model we’ve created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s_LgTbYCvUpk"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading the model and other important objects for training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the necessary objects for model fitting/training to save the time loading if met with big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from keras.models import load_model\n",
    "\n",
    "main_dir = 'model_stor\\\\'\n",
    "\n",
    "with open(main_dir+'X_train.dataset','rb') as file_stream_add:\n",
    "    X_train = pickle.load(file_stream_add)\n",
    "with open(main_dir+'y_train.dataset','rb') as file_stream_add2:\n",
    "    y_train = pickle.load(file_stream_add2)\n",
    "with open(main_dir+'X_test.dataset','rb') as file_stream_add3:\n",
    "    X_test = pickle.load(file_stream_add3)\n",
    "with open(main_dir+'y_test.dataset','rb') as file_stream_add4:\n",
    "    y_test = pickle.load(file_stream_add4)\n",
    "    \n",
    "model = load_model(main_dir+'my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJVjqkJRvbEO"
   },
   "source": [
    "## **Training the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x60O93d0vduj"
   },
   "source": [
    "In this step, we will train the model on the training set images and validate it using, you guessed it, the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2635/2635 [==============================] - 1439s 546ms/step - loss: 0.9182 - accuracy: 0.7111 - val_loss: 3.8387 - val_accuracy: 0.3074\n",
      "Epoch 2/10\n",
      "2635/2635 [==============================] - 1713s 650ms/step - loss: 0.8911 - accuracy: 0.7191 - val_loss: 3.4902 - val_accuracy: 0.3049\n",
      "Epoch 3/10\n",
      "2635/2635 [==============================] - 1665s 632ms/step - loss: 0.8678 - accuracy: 0.7279 - val_loss: 3.4826 - val_accuracy: 0.2956\n",
      "Epoch 4/10\n",
      " 539/2635 [=====>........................] - ETA: 19:55 - loss: 0.8063 - accuracy: 0.7443"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Saving the model and other important objects for training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the necessary objects for model fitting/training to save the time loading if met with big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "main_dir = 'model_stor\\\\'\n",
    "\n",
    "with open(main_dir+'X_train.dataset','wb') as file_stream_add:\n",
    "    pickle.dump(X_train, file_stream_add)\n",
    "with open(main_dir+'y_train.dataset','wb') as file_stream_add2:\n",
    "    pickle.dump(y_train, file_stream_add2)\n",
    "with open(main_dir+'X_test.dataset','wb') as file_stream_add3:\n",
    "    pickle.dump(X_test, file_stream_add3)\n",
    "with open(main_dir+'y_test.dataset','wb') as file_stream_add4:\n",
    "    pickle.dump(y_test, file_stream_add4)\n",
    "model.save(main_dir+'my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47faDOt7wE6P"
   },
   "source": [
    "## **Making predictions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIyUjI70wGlz"
   },
   "source": [
    "We’ll initially follow the steps we performed when dealing with the training data. Load the test images and predict their classes using the model.predict_classes() function. We will read and store all the test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7eChGzXwV9Y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "dim = 100\n",
    "test = pd.read_csv('test.csv')\n",
    "test_image = []\n",
    "\n",
    "for i in range(test.shape[0]):\n",
    "    img = image.load_img('test\\\\test\\\\'+test['filename'][i], grayscale=False, color_mode='grayscale', target_size=(dim,dim))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    test_image.append(img)\n",
    "    del img\n",
    "    gc.collect()\n",
    "    \n",
    "test = np.array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_image\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V6dKlyUfxFjB"
   },
   "outputs": [],
   "source": [
    "# making predictions\n",
    "prediction = model.predict_classes(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erPTR5McxU8C"
   },
   "source": [
    "Then write the results of the prediction to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "651DW_TLxfmD"
   },
   "outputs": [],
   "source": [
    "# creating results file\n",
    "results = pd.read_csv('test.csv')\n",
    "results['category'] = prediction\n",
    "results.to_csv('predicted_test.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ProductDetection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
